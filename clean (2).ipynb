{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q opacus","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T10:32:12.255471Z","iopub.execute_input":"2025-08-13T10:32:12.255819Z","iopub.status.idle":"2025-08-13T10:33:34.329214Z","shell.execute_reply.started":"2025-08-13T10:32:12.255792Z","shell.execute_reply":"2025-08-13T10:33:34.328493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Environment Setup and Imports\n\n- Imports standard libraries for file handling, timing, and numerical operations.\n- Loads PyTorch modules for model building, training, and data handling.\n- Imports scikit-learn tools for evaluation metrics and stratified cross-validation.\n- Uses `matplotlib` for plotting evaluation results.\n- Includes `opacus` for differential privacy integration.\n- Sets computation device to GPU if available, otherwise defaults to CPU.","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import (\n    confusion_matrix, ConfusionMatrixDisplay, classification_report,\n    roc_curve, auc\n)\n\nimport matplotlib.pyplot as plt\nfrom opacus import PrivacyEngine\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-13T10:33:36.888316Z","iopub.execute_input":"2025-08-13T10:33:36.888985Z","iopub.status.idle":"2025-08-13T10:33:45.153755Z","shell.execute_reply.started":"2025-08-13T10:33:36.888955Z","shell.execute_reply":"2025-08-13T10:33:45.152917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Preparation and Loading\n\n- Defines paths to training, validation, and test datasets.\n- Applies image transformations: grayscale conversion, resizing to 224×224, and tensor conversion.\n- Maps class labels explicitly: `\"NORMAL\"` → 0, `\"PNEUMONIA\"` → 1.\n- Wraps `ImageFolder` datasets with a custom class to verify and remap labels.\n- Creates `DataLoader` objects for each dataset with batch size 16 and parallel loading.\n- Prints dataset sizes and verifies label range for training data.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\n\n# ---- Set your data directory here ----\nbase_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\ntrain_dir = f\"{base_dir}/train\"\nval_dir = f\"{base_dir}/val\"\ntest_dir = f\"{base_dir}/test\"\n\n# Define transforms once\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Define label mapping explicitly (folder names must match keys exactly)\nlabel_map = {\"NORMAL\": 0, \"PNEUMONIA\": 1}\n\n# Wrapper dataset to verify and map labels (optional)\nclass CustomLabelDataset(Dataset):\n    def __init__(self, base_dataset, label_map):\n        self.base_dataset = base_dataset\n        self.label_map = label_map\n\n    def __len__(self):\n        return len(self.base_dataset)\n\n    def __getitem__(self, idx):\n        img, label = self.base_dataset[idx]\n        # ImageFolder returns int label; check mapping by folder names\n        class_name = self.base_dataset.classes[label]\n        if class_name not in self.label_map:\n            raise ValueError(f\"Unexpected class name '{class_name}' at index {idx}\")\n        mapped_label = self.label_map[class_name]\n        return img, mapped_label\n\n# Load base datasets\nbase_train_dataset = datasets.ImageFolder(train_dir, transform=transform)\nbase_val_dataset = datasets.ImageFolder(val_dir, transform=transform)\nbase_test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n\n# Wrap datasets with label mapping check (optional but recommended)\ntrain_data = CustomLabelDataset(base_train_dataset, label_map)\nval_data = CustomLabelDataset(base_val_dataset, label_map)\ntest_data = CustomLabelDataset(base_test_dataset, label_map)\n\n# Create DataLoaders after wrapping\ntrain_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_data, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_data, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Train samples: {len(train_data)}, Validation samples: {len(val_data)}, Test samples: {len(test_data)}\")\n\n# Optionally, verify labels range\nall_train_labels = [label for _, label in train_data]\nprint(f\"Train labels range: min={min(all_train_labels)}, max={max(all_train_labels)}\")\nassert min(all_train_labels) >= 0 and max(all_train_labels) < len(label_map), \"Train labels invalid\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T10:37:02.954722Z","iopub.execute_input":"2025-08-13T10:37:02.954941Z","iopub.status.idle":"2025-08-13T10:38:09.189887Z","shell.execute_reply.started":"2025-08-13T10:37:02.954924Z","shell.execute_reply":"2025-08-13T10:38:09.189005Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### `class_distribution(dataset, name)`\n- Counts and prints the number of samples per class.\n- Uses Python’s `Counter` to summarize label frequencies.\n\n### `evaluate_model_extended(model, loader, device)`\n- Sets model to evaluation mode and disables gradient computation.\n- Iterates over data loader to collect predictions, true labels, and softmax scores.\n- Prints classification report with precision, recall, and F1-score.\n- Computes and displays confusion matrix.\n- Calculates and plots ROC curve with AUC score.\n","metadata":{}},{"cell_type":"code","source":"def class_distribution(dataset, name):\n    class_counts = Counter([label for _, label in dataset])\n    print(f\"{name} class distribution: {class_counts}\")\n    return class_counts\n\ndef evaluate_model_extended(model, loader, device):\n    model.eval()\n    y_pred, y_true = [], []\n    scores = []\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            y_pred.extend(preds.cpu().numpy())\n            y_true.extend(labels.cpu().numpy())\n            probs = torch.softmax(outputs, dim=1)\n            scores.extend(probs[:, 1].cpu().numpy())\n\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=[\"NORMAL\", \"PNEUMONIA\"]))\n\n    cm = confusion_matrix(y_true, y_pred)\n    print(\"confusion_matrix\")\n    print(cm)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"NORMAL\", \"PNEUMONIA\"])\n    disp.plot(cmap='Blues')\n    plt.title(\"Confusion Matrix\")\n    plt.grid(False)\n    plt.show()\n\n    fpr, tpr, _ = roc_curve(y_true, scores)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n    plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n    plt.title(\"ROC Curve\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T10:39:04.601539Z","iopub.execute_input":"2025-08-13T10:39:04.601874Z","iopub.status.idle":"2025-08-13T10:39:04.609713Z","shell.execute_reply.started":"2025-08-13T10:39:04.601850Z","shell.execute_reply":"2025-08-13T10:39:04.608851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### `SimpleCNN` Architecture\n\n- Defines a convolutional neural network for binary image classification.\n- Uses three convolutional layers with ReLU activation and max pooling.\n- Group normalization layers replaced with identity mappings.\n- Applies dropout after each convolution and fully connected layer to reduce overfitting.\n- Flattens feature maps and passes through three fully connected layers.\n- Final layer outputs logits for two classes.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, dropout_rate=0.5):\n        super(SimpleCNN, self).__init__()\n        \n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.gn1 = nn.Identity()  # Removed GroupNorm, replaced with Identity\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.gn2 = nn.Identity()  # Removed GroupNorm\n        \n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.gn3 = nn.Identity()  # Removed GroupNorm\n        \n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Assuming input 224x224, after 3 poolings size: 28x28\n        self.fc1 = nn.Linear(128 * 28 * 28, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 2)\n        \n    def forward(self, x):\n        x = self.pool(torch.relu(self.gn1(self.conv1(x))))\n        x = self.dropout(x)\n        \n        x = self.pool(torch.relu(self.gn2(self.conv2(x))))\n        x = self.dropout(x)\n        \n        x = self.pool(torch.relu(self.gn3(self.conv3(x))))\n        x = self.dropout(x)\n        \n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        \n        x = torch.relu(self.fc2(x))\n        x = self.dropout(x)\n        \n        x = self.fc3(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T10:39:12.172084Z","iopub.execute_input":"2025-08-13T10:39:12.172547Z","iopub.status.idle":"2025-08-13T10:39:12.179809Z","shell.execute_reply.started":"2025-08-13T10:39:12.172520Z","shell.execute_reply":"2025-08-13T10:39:12.179018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Training and Cross-Validation\n\n#### `train_model(...)`\n- Trains a model for a specified number of epochs.\n- Tracks training loss and validation accuracy.\n- Evaluates model on validation set after each epoch.\n- Implements early stopping based on validation accuracy.\n- Saves and restores the best-performing model state.\n\n#### `kfold_crossval_training(...)`\n- Performs k-fold stratified cross-validation on training data.\n- Splits data into training and validation subsets for each fold.\n- Initializes a new model for each fold and applies class weighting.\n- Trains each model using the `train_model` function.\n- Returns a list of trained models, one per fold.","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15, device=None, early_stopping_patience=5, report_every=5):\n    model.train()\n    train_losses = []\n    val_accuracies = []\n    best_val_acc = 0.0\n    patience_counter = 0\n    best_model_state = None\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            if device:\n                images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        # Validation phase\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                if device:\n                    images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                val_loss += criterion(outputs, labels).item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        val_accuracy = 100 * correct / total\n        avg_train_loss = running_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n        train_losses.append(avg_train_loss)\n        val_accuracies.append(val_accuracy)\n        if ((epoch + 1) % report_every == 0) or (epoch == num_epochs - 1):\n            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n        # Early stopping logic\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= early_stopping_patience:\n                print(f\"Early stopping at epoch {epoch+1}. Best val accuracy: {best_val_acc:.2f}%\")\n                if best_model_state:\n                    model.load_state_dict(best_model_state)\n                break\n    return train_losses, val_accuracies\n\ndef kfold_crossval_training(train_data, device, k=5, batch_size=16, num_epochs=10, early_stopping_patience=5, report_every=4):\n    targets = [label for _, label in train_data]\n    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n    fold_models = []\n    for fold, (train_indices, val_indices) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n        print(f\"Fold {fold+1}/{k}\")\n        train_fold_subset = Subset(train_data, train_indices)\n        val_fold_subset = Subset(train_data, val_indices)\n        train_fold_loader = DataLoader(train_fold_subset, batch_size=batch_size, shuffle=True)\n        val_fold_loader = DataLoader(val_fold_subset, batch_size=batch_size, shuffle=False)\n        model = SimpleCNN(dropout_rate=0.3).to(device)\n        class_counts = Counter([train_data[idx][1] for idx in train_indices])\n        total_train = sum(class_counts.values())\n        class_weights = [total_train / class_counts[i] if class_counts[i] > 0 else 0.0 for i in range(len(class_counts))]\n        weights_tensor = torch.FloatTensor(class_weights).to(device)\n        criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n        train_model(model, train_fold_loader, val_fold_loader, criterion, optimizer, num_epochs, device, early_stopping_patience, report_every)\n        fold_models.append(model)\n    return fold_models\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T10:39:20.815482Z","iopub.execute_input":"2025-08-13T10:39:20.816254Z","iopub.status.idle":"2025-08-13T10:39:20.827743Z","shell.execute_reply.started":"2025-08-13T10:39:20.816226Z","shell.execute_reply":"2025-08-13T10:39:20.826930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### `ensemble_predict(...)`\n\n- Performs prediction using an ensemble of trained models.\n- Applies softmax to each model’s outputs to obtain class probabilities.\n- Aggregates predictions by averaging probabilities across models.\n- Returns final class predictions based on highest mean probability.\n","metadata":{}},{"cell_type":"code","source":"def ensemble_predict(models, dataloader, device):\n    all_probs = []\n    for model in models:\n        model.eval()\n        probs = []\n        with torch.no_grad():\n            for images, _ in dataloader:\n                images = images.to(device)\n                outputs = model(images)\n                probs.append(torch.softmax(outputs, dim=1).cpu().numpy())\n        all_probs.append(np.concatenate(probs, axis=0))\n    mean_probs = np.mean(all_probs, axis=0)\n    preds = np.argmax(mean_probs, axis=1)\n    return preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T10:39:25.456891Z","iopub.execute_input":"2025-08-13T10:39:25.457168Z","iopub.status.idle":"2025-08-13T10:39:25.462486Z","shell.execute_reply.started":"2025-08-13T10:39:25.457145Z","shell.execute_reply":"2025-08-13T10:39:25.461656Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ensemble Evaluation\n\n- Trains multiple models using 5-fold cross-validation.\n- Applies ensemble prediction by averaging outputs from all fold models.\n- Evaluates ensemble performance on the test set using accuracy score.\n- Prints final test accuracy of the ensemble classifier.\n","metadata":{}},{"cell_type":"code","source":"# Example: Standard k-fold training (5 folds)\nfold_models = kfold_crossval_training(train_data, device, k=5, batch_size=16, num_epochs=10)\n\n# Ensemble on test set\ntest_preds = ensemble_predict(fold_models, test_loader, device)\n\n# Evaluate ensemble on test set\nfrom sklearn.metrics import accuracy_score\ntrue_labels = [label for _, label in test_data]\nprint(\"Ensemble Test Accuracy:\", accuracy_score(true_labels, test_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T10:39:31.866138Z","iopub.execute_input":"2025-08-13T10:39:31.866721Z","iopub.status.idle":"2025-08-13T11:45:15.724659Z","shell.execute_reply.started":"2025-08-13T10:39:31.866696Z","shell.execute_reply":"2025-08-13T11:45:15.723733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Rationale for K-Fold Cross-Validation and Ensemble Training\n\n- **K-Fold Cross-Validation**:\n  - Ensures robust model evaluation by training on multiple train/validation splits.\n  - Reduces variance associated with a single train/test split.\n  - Helps assess model generalizability across different subsets of data.\n\n- **Ensemble Prediction**:\n  - Combines predictions from multiple models to reduce overfitting and improve stability.\n  - Averages class probabilities to mitigate individual model biases.\n  - Often yields better performance than any single model alone, especially on unseen data.\n\n- This approach balances **model reliability** and **predictive accuracy**, making it suitable for medical imaging tasks where generalization is critical.\n","metadata":{}},{"cell_type":"markdown","source":"### Ensemble Evaluation with Comprehensive Metrics\n\n- Defines `EnsembleModel` to average softmax outputs from multiple trained models.\n- Evaluates ensemble predictions on the test set using key classification metrics:\n  - **Classification report**: Precision, recall, F1-score per class.\n  - **Accuracy**: Overall prediction correctness.\n  - **Confusion matrix**: Visual breakdown of true vs. predicted labels.\n  - **ROC curve and AUC**: Measures model’s ability to distinguish between classes.\n  - **Recall and F1-score**: Focused on class 1 (PNEUMONIA), critical for medical diagnosis.\n- Returns all metrics for further analysis or reporting.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, accuracy_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# If you haven't already defined this:\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super().__init__()\n        self.models = nn.ModuleList(models)\n        for m in self.models:\n            m.eval()\n    def forward(self, x):\n        outputs = [torch.softmax(m(x), dim=1) for m in self.models]\n        return torch.mean(torch.stack(outputs), dim=0)\n\ndef evaluate_all_metrics(model, loader, device, class_names=['NORMAL', 'PNEUMONIA']):\n    model.eval()\n    y_true, y_pred, y_probs = [], [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            outputs = model(images)\n            probs = torch.softmax(outputs, dim=1)\n            preds = probs.argmax(dim=1).cpu().numpy()\n            y_pred.extend(preds)\n            y_true.extend(labels.cpu().numpy())\n            y_probs.extend(probs[:, 1].cpu().numpy())  # Assumes class 1 = PNEUMONIA\n\n    # 1. Classification report\n    print(\"\\n=== Classification Report ===\")\n    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n    # 2. Accuracy\n    acc = accuracy_score(y_true, y_pred)\n    print(f\"Accuracy: {acc:.4f}\")\n\n    # 3. Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    disp.plot(cmap='Blues')\n    plt.title(\"Confusion Matrix\")\n    plt.grid(False)\n    plt.show()\n\n    # 4. ROC Curve and AUC\n    fpr, tpr, _ = roc_curve(y_true, y_probs)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n    plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n    plt.title(\"ROC Curve\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n    # 5. Print useful summary\n    recall = recall_score(y_true, y_pred, pos_label=1)\n    f1 = f1_score(y_true, y_pred, pos_label=1)\n    print(f\"Recall (Sensitivity, class 1): {recall:.4f}\")\n    print(f\"F1-score (class 1): {f1:.4f}\")\n    print(f\"AUC: {roc_auc:.4f}\")\n\n    # Return all metrics if you want to save them\n    return {\n        \"accuracy\": acc,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"auc\": roc_auc\n    }\n# Build the ensemble from your k-fold models\nensemble = EnsembleModel(fold_models)\n\n# Now evaluate\nmetrics = evaluate_all_metrics(ensemble, test_loader, device, class_names=['NORMAL', 'PNEUMONIA'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T11:46:19.337541Z","iopub.execute_input":"2025-08-13T11:46:19.338316Z","iopub.status.idle":"2025-08-13T11:46:24.795270Z","shell.execute_reply.started":"2025-08-13T11:46:19.338284Z","shell.execute_reply":"2025-08-13T11:46:24.794647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4.x Model Performance on Test Set\nThe final ensemble model was evaluated on the held-out test set, comprising 624 chest X-ray images, with class distributions of 234 \"NORMAL\" and 390 \"PNEUMONIA\" cases. The model's performance is summarised in Table 4.x and Figure 4.x, which present the standard classification metrics, confusion matrix, and ROC curve.\n\nClassification Results\nThe ensemble achieved an overall test set accuracy of 78.85%, with an Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) of 0.9363, indicating strong discriminative ability between normal and pneumonia cases. The detailed classification report is as follows:\n\nClass\tPrecision\tRecall\tF1-score\tSupport\nNORMAL\t0.9636\t0.4530\t0.6163\t234\nPNEUMONIA\t0.7510\t0.9897\t0.8540\t390\naccuracy\t\t\t0.7885\t624\nmacro avg\t0.8573\t0.7214\t0.7351\t624\nweighted avg\t0.8307\t0.7885\t0.7648\t624\n\nThe recall (sensitivity) for PNEUMONIA is exceptionally high (0.9897), indicating that the model successfully identifies nearly all pneumonia cases. Conversely, the recall for NORMAL is substantially lower (0.4530), suggesting that a significant proportion of normal cases are misclassified as pneumonia.\n\nConfusion Matrix and ROC Analysis\nFigure 4.x (top) presents the confusion matrix. Of the 234 NORMAL cases, only 106 were correctly identified, while 128 were misclassified as PNEUMONIA. In contrast, the model correctly classified 386 out of 390 PNEUMONIA cases, with just 4 false negatives. This pattern reflects the model’s strong tendency to favour sensitivity over specificity—a desirable property in many clinical screening contexts, where missing true positive cases (false negatives) may have more severe consequences than issuing false alarms (false positives).\n\nThe ROC curve (Figure 4.x, bottom) confirms this behaviour, with an AUC of 0.9363, indicating excellent overall ability to distinguish between the two classes, despite the imbalance in recall.\n\nDiscussion and Clinical Implications\nThe results demonstrate a clear trade-off between sensitivity and specificity. The model's high sensitivity ensures that nearly all pneumonia cases are detected, minimising the risk of missed diagnoses. However, the cost is a higher rate of false positives for pneumonia, potentially resulting in unnecessary follow-up examinations for healthy patients. This reflects a design choice aligned with the clinical imperative to prioritise patient safety in screening applications.\n\nNonetheless, the relatively low recall for NORMAL cases suggests the model could benefit from further optimisation, such as additional data augmentation, class rebalancing, or threshold adjustment to improve specificity without unduly sacrificing sensitivity. Future work may also explore alternative architectures, loss functions, or calibration strategies to achieve a more balanced performance.\n\nSummary\nIn summary, the developed model demonstrates strong discriminative performance, particularly in detecting pneumonia, with an AUC of 0.94 and recall of 0.99 for the PNEUMONIA class. The observed trade-off between high sensitivity and lower specificity is consistent with best practices in clinical risk mitigation, but highlights opportunities for future refinement.","metadata":{}},{"cell_type":"markdown","source":"### Membership Inference Attack (MIA) on Ensemble Model\n\n- Constructs an ensemble model from k-fold trained models.\n- Collects maximum softmax confidences for training (member) and test (non-member) samples.\n- Builds an attack dataset using confidence scores and binary membership labels.\n- Trains a logistic regression model to distinguish between member and non-member samples.\n- Evaluates attack performance using accuracy and ROC AUC.\n- Visualizes confidence distributions to inspect separability between member and non-member data.","metadata":{}},{"cell_type":"code","source":"def collect_confidences(model, loader, device):\n    \"\"\"\n    Collects the maximum softmax confidence scores for each sample in the loader.\n    Returns:\n        confidences (np.array): max softmax confidence for each sample\n        labels (np.array): ground truth labels\n    \"\"\"\n    model.eval()\n    confidences = []\n    labels = []\n    with torch.no_grad():\n        for imgs, labs in loader:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            max_conf, _ = torch.max(probs, dim=1)\n            confidences.extend(max_conf.cpu().numpy())\n            labels.extend(labs.cpu().numpy())\n    return np.array(confidences), np.array(labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T11:48:24.972169Z","iopub.execute_input":"2025-08-13T11:48:24.972447Z","iopub.status.idle":"2025-08-13T11:48:24.977800Z","shell.execute_reply.started":"2025-08-13T11:48:24.972428Z","shell.execute_reply":"2025-08-13T11:48:24.977037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Create the ensemble from your fold models\nensemble_model = EnsembleModel(fold_models).to(device)\n\n# 2. Collect confidences for train and test sets\ntrain_confidences, train_labels = collect_confidences(ensemble_model, train_loader, device)\ntest_confidences, test_labels = collect_confidences(ensemble_model, test_loader, device)\n\n# 3. Prepare attack dataset\nmia_X = np.concatenate([train_confidences, test_confidences])[:, None]\nmia_y = np.concatenate([np.ones(len(train_confidences)), np.zeros(len(test_confidences))])\n\n# 4. Attack model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nattack_model = LogisticRegression(solver=\"lbfgs\")\nattack_model.fit(mia_X, mia_y)\nmia_preds = attack_model.predict(mia_X)\nmia_probs = attack_model.predict_proba(mia_X)[:, 1]\nprint(f\"Attack Accuracy: {accuracy_score(mia_y, mia_preds):.3f}\")\nprint(f\"Attack ROC AUC: {roc_auc_score(mia_y, mia_probs):.3f}\")\n\n# 5. Plot for inspection\nimport matplotlib.pyplot as plt\nplt.hist(train_confidences, bins=50, alpha=0.5, label=\"Member (Train)\")\nplt.hist(test_confidences, bins=50, alpha=0.5, label=\"Non-member (Test)\")\nplt.xlabel(\"Max Softmax Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"Confidence Distributions: Ensemble MIA\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T11:48:27.352780Z","iopub.execute_input":"2025-08-13T11:48:27.353049Z","iopub.status.idle":"2025-08-13T11:49:10.017219Z","shell.execute_reply.started":"2025-08-13T11:48:27.353029Z","shell.execute_reply":"2025-08-13T11:49:10.016394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Discussion: Membership Inference Attack Results\nThe robustness of machine learning models to privacy attacks is increasingly important, particularly in sensitive domains such as medical imaging. In this project, a Membership Inference Attack (MIA) was implemented to evaluate the privacy leakage of a convolutional neural network (CNN) ensemble, trained on the chest X-ray pneumonia dataset. The goal was to determine whether the trained model’s predictions reveal membership status (i.e., whether a particular record was used during training), a key concern for patient privacy.\n\nInterpretation of MIA Results\nThe MIA, implemented using the “confidence threshold” method, yielded an attack accuracy of 0.893 but an ROC AUC of 0.515. The histogram of softmax confidence distributions for both training (member) and test (non-member) samples demonstrated near-complete overlap, with almost all samples (regardless of set membership) receiving similar maximum predicted probabilities from the ensemble model.\n\nWhile the attack’s accuracy appears high, this is misleading due to class imbalance: there are substantially more member samples (training data) than non-members (test data). As a result, the attack classifier can achieve high accuracy by naively predicting \"member\" for most samples. The more appropriate metric in this context is the ROC AUC, which is threshold-independent and measures the ability to distinguish between member and non-member samples. The ROC AUC of 0.515, effectively equivalent to random guessing, indicates that the MIA was unable to reliably distinguish between training and test samples.\n\nImplications for Model Privacy\nThis result demonstrates that the ensemble model provides strong privacy protection against this class of attack. The similar confidence scores for member and non-member samples suggest that the ensemble is well-calibrated and does not overfit to the training data. In privacy terms, this means that the model does not memorize specific training instances in a way that can be exploited by an adversary, thereby significantly reducing the risk of privacy leakage (Shokri et al., 2017).\n\nThe effectiveness of the ensemble in mitigating membership inference is likely attributable to several factors:\n\nAveraging predictions over multiple models (each trained on different data splits) inherently reduces variance and overfitting, both of which are known to increase MIA risk (Salem et al., 2019).\n\nThe model training procedure incorporated early stopping and regularization, further limiting the model’s tendency to memorize the training set.\n\nThe data augmentation and class rebalancing steps during preprocessing may also have contributed to more uniform model outputs across samples.\n\nJustification and Best Practice\nIt is important to emphasize that MIA vulnerability is strongly correlated with model overfitting and overconfident predictions. In models where the maximum softmax confidence is substantially higher for member samples than for non-members, a basic MIA achieves a much higher AUC. That this was not observed here supports the effectiveness of ensemble methods for privacy risk mitigation in deep learning.\n\nWhile these results are encouraging, they do not imply immunity to all forms of privacy attacks. More sophisticated MIAs—those leveraging model loss, full softmax output vectors, or adversarially-trained attack models—may yield different outcomes. However, this evaluation provides strong initial evidence that ensemble models, trained with appropriate regularization and evaluated with proper metrics (AUC, not accuracy), can simultaneously achieve high utility and robust privacy.","metadata":{}},{"cell_type":"markdown","source":"### THINK I CAN DELETE THE CELL BELOW","metadata":{}},{"cell_type":"code","source":"# OLD CODE FOR DP TRAINING \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import Counter\nfrom opacus import PrivacyEngine\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# ------------------ DP-compatible CNN ------------------\n\nclass SimpleCNN_DP(nn.Module):\n    def __init__(self):\n        super(SimpleCNN_DP, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        self.fc1 = nn.Linear(128 * 28 * 28, 128)  # assuming input size 224x224\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 2)\n        \n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.pool(torch.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# ------------------ Gradient Magnitude Analysis ------------------\n\ndef analyze_gradient_magnitudes(model, train_loader, device, num_batches=5):\n    model.train()\n    gradient_norms = []\n    print(\"Analyzing gradient magnitudes...\")\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        if batch_idx >= num_batches:\n            break\n            \n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = nn.CrossEntropyLoss()(outputs, labels)        \n        model.zero_grad()\n        loss.backward()        \n        total_norm = 0\n        for p in model.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        total_norm = total_norm ** 0.5\n        gradient_norms.append(total_norm)     \n        print(f\"Batch {batch_idx + 1}: Gradient norm = {total_norm:.4f}\")\n    \n    avg_grad_norm = np.mean(gradient_norms)\n    max_grad_norm = np.max(gradient_norms)\n    min_grad_norm = np.min(gradient_norms)    \n    print(f\"\\nGradient Norm Statistics:\")\n    print(f\"Average: {avg_grad_norm:.4f}\")\n    print(f\"Maximum: {max_grad_norm:.4f}\")\n    print(f\"Minimum: {min_grad_norm:.4f}\")\n    \n    print(f\"\\nDP Parameter Recommendations:\")\n    print(f\"Suggested max_grad_norm values: {[avg_grad_norm * 0.5, avg_grad_norm, avg_grad_norm * 2]}\")\n    print(f\"For noise_multiplier in [0.001, 0.01, 0.1]:\")\n    for noise_mult in [0.001, 0.01, 0.1]:\n        effective_noise = noise_mult * avg_grad_norm\n        print(f\"  noise_multiplier={noise_mult} → effective noise ≈ {effective_noise:.6f}\")\n    \n    return {\n        'gradient_norms': gradient_norms,\n        'avg_norm': avg_grad_norm,\n        'max_norm': max_grad_norm,\n        'min_norm': min_grad_norm\n    }\n\n# ------------------ DP Training Loop ------------------\n\ndef train_model_dp(model, train_loader, val_loader, criterion, optimizer, device,\n                   noise_multiplier=0.1, max_grad_norm=1.0, num_epochs=15, \n                   early_stopping_patience=5, report_every=5):\n    privacy_engine = PrivacyEngine()\n    model, optimizer, train_loader = privacy_engine.make_private(\n        module=model,\n        optimizer=optimizer,\n        data_loader=train_loader,\n        noise_multiplier=noise_multiplier,\n        max_grad_norm=max_grad_norm,\n        poisson_sampling=False,\n    )\n    print(f\"Training with DP: noise_multiplier={noise_multiplier}, max_grad_norm={max_grad_norm}\")\n    model.train()\n\n    best_val_acc = 0.0\n    patience_counter = 0\n    best_model_state = None\n    train_losses, val_accuracies = [], []\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        model.eval()\n        correct, total = 0, 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                val_loss += criterion(outputs, labels).item()\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        val_accuracy = 100 * correct / total\n        avg_train_loss = running_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n\n        train_losses.append(avg_train_loss)\n        val_accuracies.append(val_accuracy)\n\n        if ((epoch + 1) % report_every == 0) or (epoch == num_epochs - 1):\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= early_stopping_patience:\n                print(f\"Early stopping at epoch {epoch+1}. Best val accuracy: {best_val_acc:.2f}%\")\n                if best_model_state is not None:\n                    model.load_state_dict(best_model_state)\n                break\n\n        model.train()\n\n    return model, train_losses, val_accuracies\n\n\n# ------------------ DP K-fold Training ------------------\n\ndef kfold_crossval_training_dp(train_data, device, k=5, batch_size=16, num_epochs=15,\n                               noise_multiplier=0.4, max_grad_norm=1.2,\n                               early_stopping_patience=5, report_every=4):\n    targets = [label for _, label in train_data]\n    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n    fold_models = []\n    \n    for fold, (train_indices, val_indices) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n        print(f\"Fold {fold+1}/{k}\")\n        train_fold_subset = Subset(train_data, train_indices)\n        val_fold_subset = Subset(train_data, val_indices)\n        train_fold_loader = DataLoader(train_fold_subset, batch_size=batch_size, shuffle=True)\n        val_fold_loader = DataLoader(val_fold_subset, batch_size=batch_size, shuffle=False)\n        \n        model = SimpleCNN_DP().to(device)\n    \n        \n        class_counts = Counter([train_data[idx][1] for idx in train_indices])\n        total_train = sum(class_counts.values())\n        class_weights = [total_train / class_counts[i] if class_counts[i] > 0 else 0.0 for i in range(len(class_counts))]\n        weights_tensor = torch.FloatTensor(class_weights).to(device)\n        \n        criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n        \n        model, train_losses, val_accuracies = train_model_dp(\n            model=model,\n            train_loader=train_fold_loader,\n            val_loader=val_fold_loader,\n            criterion=criterion,\n            optimizer=optimizer,\n            device=device,\n            noise_multiplier=noise_multiplier,\n            max_grad_norm=max_grad_norm,\n            num_epochs=num_epochs,\n            early_stopping_patience=early_stopping_patience,\n            report_every=report_every,\n        )\n        \n        fold_models.append(model)\n        print(f\"Best val accuracy for fold {fold+1}: {max(val_accuracies):.2f}%\")\n    \n    return fold_models\n\n# ------------------ Ensemble Model ------------------\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super(EnsembleModel, self).__init__()\n        self.models = nn.ModuleList(models)\n        for model in self.models:\n            model.eval()\n    \n    def forward(self, x):\n        outputs = [torch.softmax(m(x), dim=1) for m in self.models]\n        mean_output = torch.mean(torch.stack(outputs), dim=0)\n        return mean_output\n\n# ------------------ Model Evaluation ------------------\n\ndef evaluate_model_extended(model, loader, device):\n    model.eval()\n    y_pred, y_true = [], []\n    scores = []\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            y_pred.extend(preds.cpu().numpy())\n            y_true.extend(labels.cpu().numpy())\n            probs = torch.softmax(outputs, dim=1)\n            scores.extend(probs[:, 1].cpu().numpy())\n\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=[\"NORMAL\", \"PNEUMONIA\"]))\n\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"NORMAL\", \"PNEUMONIA\"])\n    disp.plot(cmap='Blues')\n    plt.title(\"Confusion Matrix\")\n    plt.grid(False)\n    plt.show()\n\n    fpr, tpr, _ = roc_curve(y_true, scores)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n    plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n    plt.title(\"ROC Curve\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# ------------------ Collect confidences for MIA ------------------\n\ndef collect_confidences(model, loader, device):\n    model.eval()\n    confidences = []\n    labels = []\n    with torch.no_grad():\n        for images, labs in loader:\n            images = images.to(device)\n            outputs = model(images)\n            probs = torch.softmax(outputs, dim=1)\n            max_confidence, _ = torch.max(probs, dim=1)\n            confidences.extend(max_confidence.cpu().numpy())\n            labels.extend(labs.cpu().numpy())\n    return np.array(confidences), np.array(labels)\n\n\n# ------------------ ---- EXECUTION CODE ---- ------------------\n\n# Assuming these are defined:\n# train_data, train_loader, val_loader, test_loader, device\n\n# 1. Analyze gradients (optional but recommended)\nsample_model = SimpleCNN_DP().to(device)\n\ngrad_analysis = analyze_gradient_magnitudes(sample_model, train_loader, device, num_batches=5)\nmax_grad_norm = grad_analysis['avg_norm']\n\n# 2. Run DP k-fold training\nnoise_multiplier = 0.4\nfold_models_dp = kfold_crossval_training_dp(\n    train_data=train_data,\n    device=device,\n    k=5,\n    batch_size=16,\n    num_epochs=15,\n    noise_multiplier=noise_multiplier,\n    max_grad_norm=max_grad_norm,\n    early_stopping_patience=5,\n    report_every=5,\n)\n\n# 3. Build ensemble and evaluate\nensemble_model_dp = EnsembleModel(fold_models_dp).to(device)\nevaluate_model_extended(ensemble_model_dp, test_loader, device)\n\n# 4. Run MIA attack on DP ensemble\ntrain_confidences_dp, _ = collect_confidences(ensemble_model_dp, train_loader, device)\ntest_confidences_dp, _ = collect_confidences(ensemble_model_dp, test_loader, device)\nmia_X_dp = np.concatenate([train_confidences_dp, test_confidences_dp])[:, None]\nmia_y_dp = np.concatenate([np.ones(len(train_confidences_dp)), np.zeros(len(test_confidences_dp))])\n\nattack_model_dp = LogisticRegression(solver=\"lbfgs\")\nattack_model_dp.fit(mia_X_dp, mia_y_dp)\nmia_preds_dp = attack_model_dp.predict(mia_X_dp)\nmia_probs_dp = attack_model_dp.predict_proba(mia_X_dp)[:, 1]\n\nprint(f\"DP Ensemble MIA Attack Accuracy: {accuracy_score(mia_y_dp, mia_preds_dp):.3f}\")\nprint(f\"DP Ensemble MIA Attack ROC AUC: {roc_auc_score(mia_y_dp, mia_probs_dp):.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T19:49:25.909081Z","iopub.execute_input":"2025-08-04T19:49:25.909927Z","iopub.status.idle":"2025-08-04T21:34:08.147555Z","shell.execute_reply.started":"2025-08-04T19:49:25.909895Z","shell.execute_reply":"2025-08-04T21:34:08.144349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from opacus.accountants.analysis import rdp\nimport numpy as np\n\ndef compute_epsilon(noise_multiplier, sample_rate, epochs, delta=1e-5):\n    \"\"\"\n    Compute epsilon given DP parameters using Renyi DP accountant.\n\n    Args:\n      noise_multiplier: float, noise multiplier used during training\n      sample_rate: float, batch_size / total training set size\n      epochs: int, number of training epochs\n      delta: float, target delta (usually 1e-5 or 1/number_of_samples)\n\n    Returns:\n      epsilon: float, privacy budget epsilon\n    \"\"\"\n    orders = np.arange(2, 64, 0.5)\n    steps = int(epochs / sample_rate)  # number of sampling steps\n    \n    # Compute RDP values for each order\n    rdp_eps = rdp.compute_rdp(q=sample_rate, noise_multiplier=noise_multiplier, steps=steps, orders=orders)\n\n    # Compute epsilon for the given delta using keyword arguments (required by newer Opacus versions)\n    eps, opt_order = rdp.get_privacy_spent(orders=orders, rdp=rdp_eps, delta=delta)\n\n    print(f\"DP epsilon: {eps:.4f} for delta={delta} at optimal order {opt_order}\")\n    return eps\n\n# Example usage:\n\ntotal_training_samples = len(train_data)  # your dataset size\nbatch_size = 16                           # batch size used during training\nepochs = 15                              # number of training epochs\nsample_rate = batch_size / total_training_samples\ndelta = 1e-5                            # typical delta value\nnoise_multiplier = 0.05                 # your noise multiplier used in DP training\n\nepsilon = compute_epsilon(noise_multiplier, sample_rate, epochs, delta)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T21:40:27.755304Z","iopub.execute_input":"2025-08-04T21:40:27.755566Z","iopub.status.idle":"2025-08-04T21:40:27.788842Z","shell.execute_reply.started":"2025-08-04T21:40:27.755546Z","shell.execute_reply":"2025-08-04T21:40:27.787989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import Counter\nfrom opacus import PrivacyEngine\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport gc\n\n# ---- Analyze gradient magnitudes ----\ndef analyze_gradient_magnitudes(model, train_loader, device, num_batches=5):\n    model.train()\n    gradient_norms = []\n    print(\"Analyzing gradient magnitudes...\")\n    for batch_idx, (images, labels) in enumerate(train_loader):\n        if batch_idx >= num_batches:\n            break\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        model.zero_grad()\n        loss.backward()\n        total_norm = 0\n        for p in model.parameters():\n            if p.grad is not None:\n                total_norm += p.grad.data.norm(2).item() ** 2\n        total_norm = total_norm ** 0.5\n        gradient_norms.append(total_norm)\n        print(f\"Batch {batch_idx + 1}: Gradient norm = {total_norm:.4f}\")\n    avg_grad_norm = np.mean(gradient_norms)\n    print(f\"\\nGradient Norm Statistics:\")\n    print(f\"Average: {avg_grad_norm:.4f}\")\n    return {'avg_norm': avg_grad_norm}\n\n# ---- DP-compatible CNN ----\nclass SimpleCNN_DP(nn.Module):\n    def __init__(self, dropout_rate=0.3):\n        super(SimpleCNN_DP, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc1 = nn.Linear(128 * 28 * 28, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 2)\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.pool(torch.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)\n        x = self.dropout(torch.relu(self.fc1(x)))\n        x = self.dropout(torch.relu(self.fc2(x)))\n        x = self.fc3(x)\n        return x\n\n# ---- DP training function ----\ndef train_model_dp(model, train_loader, val_loader, criterion, optimizer, device,\n                   noise_multiplier=0.1, max_grad_norm=1.0, num_epochs=3, \n                   early_stopping_patience=3, report_every=1):\n    privacy_engine = PrivacyEngine()\n    model, optimizer, train_loader = privacy_engine.make_private(\n        module=model,\n        optimizer=optimizer,\n        data_loader=train_loader,\n        noise_multiplier=noise_multiplier,\n        max_grad_norm=max_grad_norm,\n        poisson_sampling=False,\n    )\n    model.train()\n    best_val_acc = 0.0\n    patience_counter = 0\n    best_model_state = None\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        scheduler.step()  # update learning rate after each epoch\n        model.eval()\n        correct, total = 0, 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                val_loss += criterion(outputs, labels).item()\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        val_accuracy = 100 * correct / total\n        avg_train_loss = running_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n        if ((epoch + 1) % report_every == 0) or (epoch == num_epochs - 1):\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= early_stopping_patience:\n                print(f\"Early stopping at epoch {epoch+1}. Best val accuracy: {best_val_acc:.2f}%\")\n                if best_model_state is not None:\n                    model.load_state_dict(best_model_state)\n                break\n        model.train()\n    return model\n\n# ---- DP k-fold training ----\ndef kfold_crossval_training_dp(train_data, device, k=3, batch_size=8, num_epochs=3,\n                               noise_multiplier=0.1, max_grad_norm=1.0,\n                               early_stopping_patience=3, report_every=1):\n    targets = [label for _, label in train_data]\n    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n    fold_models = []\n    for fold, (train_indices, val_indices) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n        print(f\"Fold {fold+1}/{k}\")\n        train_fold_subset = Subset(train_data, train_indices)\n        val_fold_subset = Subset(train_data, val_indices)\n        train_fold_loader = DataLoader(train_fold_subset, batch_size=batch_size, shuffle=True)\n        val_fold_loader = DataLoader(val_fold_subset, batch_size=batch_size, shuffle=False)\n        model = SimpleCNN_DP().to(device)\n        class_counts = Counter([train_data[idx][1] for idx in train_indices])\n        total_train = sum(class_counts.values())\n        class_weights = [total_train / class_counts[i] if class_counts[i] > 0 else 0.0 for i in range(len(class_counts))]\n        weights_tensor = torch.FloatTensor(class_weights).to(device)\n        criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n        optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n        model = train_model_dp(\n            model=model,\n            train_loader=train_fold_loader,\n            val_loader=val_fold_loader,\n            criterion=criterion,\n            optimizer=optimizer,\n            device=device,\n            noise_multiplier=noise_multiplier,\n            max_grad_norm=max_grad_norm,\n            num_epochs=num_epochs,\n            early_stopping_patience=early_stopping_patience,\n            report_every=report_every,\n        )\n        fold_models.append(model)\n        # Clear memory after each fold\n        gc.collect()\n        torch.cuda.empty_cache()\n    return fold_models\n\n# ---- Ensemble model for soft voting ----\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super(EnsembleModel, self).__init__()\n        self.models = nn.ModuleList(models)\n        for model in self.models:\n            model.eval()\n    def forward(self, x):\n        outputs = [torch.softmax(m(x), dim=1) for m in self.models]\n        mean_output = torch.mean(torch.stack(outputs), dim=0)\n        return mean_output\n\n# ---- Evaluate model (classification report + ROC curve) ----\ndef evaluate_model_extended(model, loader, device):\n    model.eval()\n    y_pred, y_true, scores = [], [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            y_pred.extend(preds.cpu().numpy())\n            y_true.extend(labels.cpu().numpy())\n            probs = torch.softmax(outputs, dim=1)\n            scores.extend(probs[:,1].cpu().numpy())\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=[\"NORMAL\", \"PNEUMONIA\"]))\n    fpr, tpr, _ = roc_curve(y_true, scores)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n    plt.plot([0,1],[0,1],'--', color='gray')\n    plt.title(\"ROC Curve\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend()\n    plt.show()\n\n# ---- Collect max softmax confidence for MIA ----\ndef collect_confidences(model, loader, device):\n    model.eval()\n    confidences, labels = [], []\n    with torch.no_grad():\n        for images, labs in loader:\n            images = images.to(device)\n            outputs = model(images)\n            probs = torch.softmax(outputs, dim=1)\n            max_confidence, _ = torch.max(probs, dim=1)\n            confidences.extend(max_confidence.cpu().numpy())\n            labels.extend(labs.cpu().numpy())\n    return np.array(confidences), np.array(labels)\n\n# ---- Main grid search with MIA attack ----\ndef grid_search_with_mia(train_data, train_loader, val_loader, test_loader, device, avg_grad_norm):\n    noise_multipliers = [0.001, 0.01, 0.05]\n    max_grad_norms = [avg_grad_norm * 0.5, avg_grad_norm, avg_grad_norm * 2]\n    results = []\n    for noise_mult in noise_multipliers:\n        for max_grad in max_grad_norms:\n            print(f\"\\nGrid search - noise_multiplier={noise_mult}, max_grad_norm={max_grad:.4f}\")\n            fold_models = kfold_crossval_training_dp(\n                train_data=train_data,\n                device=device,\n                k=3,\n                batch_size=8,\n                num_epochs=3,\n                noise_multiplier=noise_mult,\n                max_grad_norm=max_grad,\n                early_stopping_patience=3,\n                report_every=1,\n            )\n            ensemble_model = EnsembleModel(fold_models).to(device)\n            # Evaluate utility\n            evaluate_model_extended(ensemble_model, test_loader, device)\n            # Collect confidences for MIA attack\n            train_confidences, _ = collect_confidences(ensemble_model, train_loader, device)\n            test_confidences, _ = collect_confidences(ensemble_model, test_loader, device)\n            mia_X = np.concatenate([train_confidences, test_confidences])[:, None]\n            mia_y = np.concatenate([np.ones(len(train_confidences)), np.zeros(len(test_confidences))])\n            attack_model = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n            attack_model.fit(mia_X, mia_y)\n            mia_preds = attack_model.predict(mia_X)\n            mia_probs = attack_model.predict_proba(mia_X)[:, 1]\n            mia_acc = accuracy_score(mia_y, mia_preds)\n            mia_auc = roc_auc_score(mia_y, mia_probs)\n            print(f\"MIA Attack Accuracy: {mia_acc:.3f}, ROC AUC: {mia_auc:.3f}\")\n            results.append({\n                \"noise_multiplier\": noise_mult,\n                \"max_grad_norm\": max_grad,\n                \"mia_attack_acc\": mia_acc,\n                \"mia_attack_auc\": mia_auc\n            })\n            # Clear memory explicitly after each grid point\n            gc.collect()\n            torch.cuda.empty_cache()\n    # Save results to CSV\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(\"dp_grid_search_results.csv\", index=False)\n    print(\"Grid search complete. Results saved to dp_grid_search_results.csv\")\n    # Plot results\n    fig, ax1 = plt.subplots()\n    ax1.set_xlabel('Noise Multiplier')\n    ax1.set_ylabel('MIA Attack Accuracy', color='tab:red')\n    ax1.plot(results_df['noise_multiplier'], results_df['mia_attack_acc'], 'o-', color='tab:red', label='MIA Attack Accuracy')\n    ax1.tick_params(axis='y', labelcolor='tab:red')\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('MIA Attack ROC AUC', color='tab:blue')\n    ax2.plot(results_df['noise_multiplier'], results_df['mia_attack_auc'], 's--', color='tab:blue', label='MIA Attack ROC AUC')\n    ax2.tick_params(axis='y', labelcolor='tab:blue')\n    plt.title(\"MIA Attack Metrics vs Noise Multiplier\")\n    fig.tight_layout()\n    plt.legend(loc=\"upper left\")\n    plt.show()\n\n# Example usage:\nsample_model = SimpleCNN_DP().to(device)\ngrad_analysis = analyze_gradient_magnitudes(sample_model, train_loader, device, num_batches=5)\navg_grad_norm = grad_analysis['avg_norm']\ngrid_search_with_mia(train_data, train_loader, val_loader, test_loader, device, avg_grad_norm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T11:49:38.305851Z","iopub.execute_input":"2025-08-13T11:49:38.306540Z","iopub.status.idle":"2025-08-13T14:18:04.607542Z","shell.execute_reply.started":"2025-08-13T11:49:38.306514Z","shell.execute_reply":"2025-08-13T14:18:04.606742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Differentially Private Training with ResNet18\n\n- Loads and preprocesses chest X-ray images for binary classification.\n- Uses pretrained ResNet18 with frozen layers; replaces and trains final fully connected layer.\n- Applies grayscale-to-RGB conversion to match ResNet input requirements.\n- Integrates `Opacus` PrivacyEngine to enforce differential privacy during training:\n  - Adds calibrated noise to gradients.\n  - Clips gradients to bound sensitivity.\n  - Tracks privacy budget (ε, δ) across epochs.\n- Trains model using cross-entropy loss and Adam optimizer.\n- Evaluates model performance using classification report and ROC curve on validation set.","metadata":{}},{"cell_type":"markdown","source":"Below code basically an earlier/training-only script that leans on the tiny val/ folder and lacks MIA + final test reporting. You don’t need it for the dissertation.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom opacus import PrivacyEngine\nimport numpy as np\nfrom sklearn.metrics import classification_report, roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom torchvision.models import ResNet18_Weights\n\n# Dataset paths\nbase_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\ntrain_dir = f\"{base_dir}/train\"\nval_dir = f\"{base_dir}/val\"\ntest_dir = f\"{base_dir}/test\"\n\n# Transforms (224x224, grayscale converted to 3-channel for ResNet)\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),  # ResNet expects 3 channels\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Load datasets\ntrain_dataset = datasets.ImageFolder(train_dir, transform=transform)\nval_dataset = datasets.ImageFolder(val_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(test_dir, transform=transform)\n\nbatch_size = 32  # adjust if needed\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load pretrained ResNet18 with updated API, freeze all layers first\nmodel = models.resnet18(weights=ResNet18_Weights.DEFAULT)\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final fully connected layer (unfreeze it)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 2)  # 2 classes: NORMAL, PNEUMONIA\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n\n# Initialize PrivacyEngine correctly (no args in constructor)\nprivacy_engine = PrivacyEngine()\n\nmodel, optimizer, train_loader = privacy_engine.make_private(\n    module=model,\n    optimizer=optimizer,\n    data_loader=train_loader,\n    noise_multiplier=1.0,   # tune noise for privacy-utility tradeoff\n    max_grad_norm=1.0,\n    epochs=10,              # total epochs for privacy accounting\n)\n\ndef evaluate(model, loader):\n    model.eval()\n    y_true, y_pred, y_probs = [], [], []\n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n            y_probs.extend(probs[:,1].cpu().numpy())\n    print(classification_report(y_true, y_pred, target_names=train_dataset.classes))\n    fpr, tpr, _ = roc_curve(y_true, y_probs)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n    plt.plot([0,1], [0,1], '--', color='gray')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend()\n    plt.show()\n\n# Training loop\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    avg_loss = running_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n    \n    epsilon = privacy_engine.get_epsilon(delta=1e-5)\n    print(f\"Privacy budget spent: ε = {epsilon:.2f}, δ=1e-5\")\n    \n    print(\"Validation results:\")\n    evaluate(model, val_loader)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:36:02.529748Z","iopub.execute_input":"2025-08-13T17:36:02.530080Z","iopub.status.idle":"2025-08-13T17:46:10.561252Z","shell.execute_reply.started":"2025-08-13T17:36:02.530054Z","shell.execute_reply":"2025-08-13T17:46:10.560411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------- Final Test Evaluation ---------\nprint(\"Final test results:\")\nevaluate(model, test_loader)\n\n# --------- Collect confidence scores for MIA ---------\ndef collect_confidences(model, loader):\n    model.eval()\n    confidences = []\n    labels = []\n    with torch.no_grad():\n        for imgs, labs in loader:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            max_confidence, _ = torch.max(probs, dim=1)\n            confidences.extend(max_confidence.cpu().numpy())\n            labels.extend(labs.cpu().numpy())\n    return np.array(confidences), np.array(labels)\n\n# --------- MIA attack simulation ---------\nprint(\"Running Membership Inference Attack (MIA) simulation...\")\n\ntrain_confidences, _ = collect_confidences(model, train_loader)\ntest_confidences, _ = collect_confidences(model, test_loader)\n\n# Labels: 1 for members (train), 0 for non-members (test)\nmia_X = np.concatenate([train_confidences, test_confidences])[:, None]\nmia_y = np.concatenate([np.ones(len(train_confidences)), np.zeros(len(test_confidences))])\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nattack_model = LogisticRegression(solver='lbfgs', max_iter=1000)\nattack_model.fit(mia_X, mia_y)\nmia_preds = attack_model.predict(mia_X)\nmia_probs = attack_model.predict_proba(mia_X)[:, 1]\n\nmia_acc = accuracy_score(mia_y, mia_preds)\nmia_auc = roc_auc_score(mia_y, mia_probs)\n\nprint(f\"MIA Attack Accuracy: {mia_acc:.3f}\")\nprint(f\"MIA Attack ROC AUC: {mia_auc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:07:05.522126Z","iopub.execute_input":"2025-08-13T18:07:05.522990Z","iopub.status.idle":"2025-08-13T18:08:13.415079Z","shell.execute_reply.started":"2025-08-13T18:07:05.522961Z","shell.execute_reply":"2025-08-13T18:08:13.414203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Differentially Private Training and Membership Inference Attack (MIA)\n\n#### Data Preparation\n- Loads chest X-ray images from Kaggle dataset.\n- Applies preprocessing: grayscale to RGB conversion, resizing to 224×224, and tensor conversion.\n- Splits training data into 80% training and 20% validation.\n- Creates DataLoaders for training, validation, and test sets.\n\n#### Model Setup\n- Loads pretrained ResNet18 and freezes all layers except the final fully connected layer.\n- Replaces the final layer to output logits for two classes: NORMAL and PNEUMONIA.\n- Moves model to GPU if available.\n\n#### Differential Privacy Integration\n- Uses `Opacus` PrivacyEngine to apply differentially private stochastic gradient descent (DP-SGD).\n- Adds noise to gradients and clips them to limit sensitivity.\n- Tracks privacy budget (ε, δ) across training epochs.\n\n#### Training Loop\n- Trains the model for 10 epochs using DP-SGD.\n- Computes and prints average training loss per epoch.\n- Reports privacy budget spent (ε) after each epoch.\n- Evaluates model on validation set using classification metrics and ROC curve.\n\n#### Final Evaluation\n- Evaluates model performance on the held-out test set using classification report and ROC AUC.\n\n#### Membership Inference Attack (MIA)\n- Collects maximum softmax confidence scores from training and test sets.\n- Constructs an attack dataset: confidence scores as features, membership labels (1 for train, 0 for test).\n- Trains a logistic regression model to predict membership status.\n- Evaluates attack success using accuracy and ROC AUC.\n- High attack performance may indicate privacy leakage from the trained model.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nfrom opacus import PrivacyEngine\nimport numpy as np\nfrom sklearn.metrics import classification_report, roc_curve, auc, accuracy_score, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom torchvision.models import ResNet18_Weights\n\n# Dataset paths\nbase_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\ntrain_dir = f\"{base_dir}/train\"\ntest_dir = f\"{base_dir}/test\"\n\n# Transforms (224x224, grayscale converted to 3-channel for ResNet)\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),  # ResNet expects 3 channels\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Load full training dataset (will split into train + val)\nfull_train_dataset = datasets.ImageFolder(train_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(test_dir, transform=transform)\n\n# Split full training set into 80% train, 20% validation\ntrain_size = int(0.8 * len(full_train_dataset))\nval_size = len(full_train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\nprint(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\nprint(f\"Classes: {full_train_dataset.classes}\")\n\nbatch_size = 32  # adjust based on your GPU\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load pretrained ResNet18 and freeze all layers\nmodel = models.resnet18(weights=ResNet18_Weights.DEFAULT)\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final FC layer and unfreeze it\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 2)\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n\n# Setup Privacy Engine\nprivacy_engine = PrivacyEngine()\nmodel, optimizer, train_loader = privacy_engine.make_private(\n    module=model,\n    optimizer=optimizer,\n    data_loader=train_loader,\n    noise_multiplier=1.0,   # Tune for your privacy-utility tradeoff\n    max_grad_norm=1.0,\n    epochs=10,              # total epochs for privacy accounting\n)\n\ndef evaluate(model, loader):\n    model.eval()\n    y_true, y_pred, y_probs = [], [], []\n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n            y_probs.extend(probs[:,1].cpu().numpy())\n    print(classification_report(y_true, y_pred, target_names=full_train_dataset.classes))\n    fpr, tpr, _ = roc_curve(y_true, y_probs)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n    plt.plot([0,1], [0,1], '--', color='gray')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend()\n    plt.show()\n\n# --------- Collect confidence scores for MIA ---------\ndef collect_confidences(model, loader):\n    model.eval()\n    confidences = []\n    labels = []\n    with torch.no_grad():\n        for imgs, labs in loader:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            max_confidence, _ = torch.max(probs, dim=1)\n            confidences.extend(max_confidence.cpu().numpy())\n            labels.extend(labs.cpu().numpy())\n    return np.array(confidences), np.array(labels)\n\n# Training loop with DP\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    avg_loss = running_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n\n    epsilon = privacy_engine.get_epsilon(delta=1e-5)\n    print(f\"Privacy budget spent: ε = {epsilon:.2f}, δ=1e-5\")\n\n    print(\"Validation results:\")\n    evaluate(model, val_loader)\n\nprint(\"Final test results:\")\nevaluate(model, test_loader)\n\n# --------- MIA attack simulation ---------\nprint(\"Running Membership Inference Attack (MIA) simulation...\")\n\ntrain_confidences, _ = collect_confidences(model, train_loader)\ntest_confidences, _ = collect_confidences(model, test_loader)\n\n# Labels: 1 for members (train), 0 for non-members (test)\nmia_X = np.concatenate([train_confidences, test_confidences])[:, None]\nmia_y = np.concatenate([np.ones(len(train_confidences)), np.zeros(len(test_confidences))])\n\nfrom sklearn.linear_model import LogisticRegression\n\nattack_model = LogisticRegression(solver='lbfgs', max_iter=1000)\nattack_model.fit(mia_X, mia_y)\nmia_preds = attack_model.predict(mia_X)\nmia_probs = attack_model.predict_proba(mia_X)[:, 1]\n\nmia_acc = accuracy_score(mia_y, mia_preds)\nmia_auc = roc_auc_score(mia_y, mia_probs)\n\nprint(f\"MIA Attack Accuracy: {mia_acc:.3f}\")\nprint(f\"MIA Attack ROC AUC: {mia_auc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:53:02.435540Z","iopub.execute_input":"2025-08-13T17:53:02.435893Z","iopub.status.idle":"2025-08-13T18:04:42.102787Z","shell.execute_reply.started":"2025-08-13T17:53:02.435868Z","shell.execute_reply":"2025-08-13T18:04:42.101974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Code In box below runs the grid search - takes 4 hours","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nfrom opacus import PrivacyEngine\nimport numpy as np\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nimport gc\n\n# Dataset paths and transforms\nbase_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\ntrain_dir = f\"{base_dir}/train\"\ntest_dir = f\"{base_dir}/test\"\n\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Load datasets and split train/val\nfull_train_dataset = datasets.ImageFolder(train_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(test_dir, transform=transform)\n\ntrain_size = int(0.8 * len(full_train_dataset))\nval_size = len(full_train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\nbatch_size = 32\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef create_data_loaders(batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    return train_loader, val_loader, test_loader\n\ndef build_model():\n    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n    for param in model.parameters():\n        param.requires_grad = False\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, 2)\n    for param in model.fc.parameters():\n        param.requires_grad = True\n    return model.to(device)\n\ndef train_and_evaluate(noise_multiplier, max_grad_norm, learning_rate, epochs=10):\n    print(f\"\\nStarting training with noise_multiplier={noise_multiplier}, max_grad_norm={max_grad_norm}, learning_rate={learning_rate}\")\n    \n    train_loader, val_loader, test_loader = create_data_loaders(batch_size)\n    \n    model = build_model()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n\n    privacy_engine = PrivacyEngine()\n    model, optimizer, train_loader = privacy_engine.make_private(\n        module=model,\n        optimizer=optimizer,\n        data_loader=train_loader,\n        noise_multiplier=noise_multiplier,\n        max_grad_norm=max_grad_norm,\n        epochs=epochs,\n    )\n\n    best_val_acc = 0\n    best_model_state = None\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / len(train_loader)\n\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                outputs = model(imgs)\n                preds = outputs.argmax(dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        val_acc = correct / total\n\n        epsilon = privacy_engine.get_epsilon(delta=1e-5)\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Val Acc: {val_acc:.4f} - ε: {epsilon:.2f}\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_state = model.state_dict()\n\n    # Load best model for test evaluation\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n\n    # Test evaluation\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            preds = outputs.argmax(dim=1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n    test_acc = accuracy_score(y_true, y_pred)\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n\n    # Collect confidence scores for MIA\n    train_confidences = collect_confidences(model, train_loader)\n    test_confidences = collect_confidences(model, test_loader)\n\n    mia_X = np.concatenate([train_confidences, test_confidences])[:, None]\n    mia_y = np.concatenate([np.ones(len(train_confidences)), np.zeros(len(test_confidences))])\n    attack_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n    attack_model.fit(mia_X, mia_y)\n    mia_preds = attack_model.predict(mia_X)\n    mia_probs = attack_model.predict_proba(mia_X)[:, 1]\n    mia_acc = accuracy_score(mia_y, mia_preds)\n    mia_auc = roc_auc_score(mia_y, mia_probs)\n    print(f\"MIA Attack Accuracy: {mia_acc:.3f}, ROC AUC: {mia_auc:.3f}\")\n\n    # Clear memory to avoid crashes\n    del model, optimizer, privacy_engine, train_loader, val_loader, test_loader\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return val_acc, epsilon, mia_acc, mia_auc, noise_multiplier, max_grad_norm, learning_rate\n\ndef collect_confidences(model, loader):\n    model.eval()\n    confidences = []\n    with torch.no_grad():\n        for imgs, _ in loader:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            max_confidence, _ = torch.max(probs, dim=1)\n            confidences.extend(max_confidence.cpu().numpy())\n    return np.array(confidences)\n\n# Grid search parameter options\nnoise_multipliers = [0.5, 1.0, 1.5]\nmax_grad_norms = [0.5, 1.0, 2.0]\nlearning_rates = [1e-3, 5e-4]\n\nresults = []\n\nfor nm in noise_multipliers:\n    for mg in max_grad_norms:\n        for lr in learning_rates:\n            val_acc, epsilon, mia_acc, mia_auc, nm_, mg_, lr_ = train_and_evaluate(nm, mg, lr)\n            results.append({\n                \"val_accuracy\": val_acc,\n                \"epsilon\": epsilon,\n                \"mia_attack_accuracy\": mia_acc,\n                \"mia_attack_auc\": mia_auc,\n                \"noise_multiplier\": nm_,\n                \"max_grad_norm\": mg_,\n                \"learning_rate\": lr_,\n            })\n\n# Save results\nimport pandas as pd\ndf_results = pd.DataFrame(results)\ndf_results = df_results.sort_values(by=[\"val_accuracy\", \"epsilon\"], ascending=[False, True])\ndf_results.to_csv(\"dp_grid_search_results.csv\", index=False)\nprint(\"Grid search complete. Results saved to dp_grid_search_results.csv\")\nprint(df_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T15:28:23.530073Z","iopub.execute_input":"2025-08-05T15:28:23.530483Z","iopub.status.idle":"2025-08-05T19:01:00.777214Z","shell.execute_reply.started":"2025-08-05T15:28:23.530449Z","shell.execute_reply":"2025-08-05T19:01:00.776466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Section Below breaks down the cell above so I don't need to re run the grid search again","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nfrom opacus import PrivacyEngine\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nimport gc\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:16:29.436935Z","iopub.execute_input":"2025-08-13T18:16:29.437254Z","iopub.status.idle":"2025-08-13T18:16:29.442537Z","shell.execute_reply.started":"2025-08-13T18:16:29.437232Z","shell.execute_reply":"2025-08-13T18:16:29.441809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset paths and transforms\nbase_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\ntrain_dir = f\"{base_dir}/train\"\ntest_dir = f\"{base_dir}/test\"\n\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Load datasets and split train/val\nfull_train_dataset = datasets.ImageFolder(train_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(test_dir, transform=transform)\n\ntrain_size = int(0.8 * len(full_train_dataset))\nval_size = len(full_train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\nbatch_size = 32\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef create_data_loaders(batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    return train_loader, val_loader, test_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:16:34.077497Z","iopub.execute_input":"2025-08-13T18:16:34.078235Z","iopub.status.idle":"2025-08-13T18:16:36.260213Z","shell.execute_reply.started":"2025-08-13T18:16:34.078209Z","shell.execute_reply":"2025-08-13T18:16:36.259418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model():\n    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n    for param in model.parameters():\n        param.requires_grad = False\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, 2)\n    for param in model.fc.parameters():\n        param.requires_grad = True\n    return model.to(device)\n\ndef collect_confidences(model, loader):\n    model.eval()\n    confidences = []\n    with torch.no_grad():\n        for imgs, _ in loader:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            max_confidence, _ = torch.max(probs, dim=1)\n            confidences.extend(max_confidence.cpu().numpy())\n    return np.array(confidences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:16:39.541374Z","iopub.execute_input":"2025-08-13T18:16:39.541957Z","iopub.status.idle":"2025-08-13T18:16:39.547433Z","shell.execute_reply.started":"2025-08-13T18:16:39.541932Z","shell.execute_reply":"2025-08-13T18:16:39.546587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_evaluate(noise_multiplier, max_grad_norm, learning_rate, epochs=15):\n    print(f\"\\nStarting training with noise_multiplier={noise_multiplier}, max_grad_norm={max_grad_norm}, learning_rate={learning_rate}\")\n    \n    train_loader, val_loader, test_loader = create_data_loaders(batch_size)\n    \n    model = build_model()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n\n    # Use secure_mode=False so it works in Kaggle (no need to specify, as it's default)\n    privacy_engine = PrivacyEngine(secure_mode=False)\n    model, optimizer, train_loader = privacy_engine.make_private(\n        module=model,\n        optimizer=optimizer,\n        data_loader=train_loader,\n        noise_multiplier=noise_multiplier,\n        max_grad_norm=max_grad_norm,\n        epochs=epochs,\n    )\n\n    best_val_acc = 0\n    best_model_state = None\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / len(train_loader)\n\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                outputs = model(imgs)\n                preds = outputs.argmax(dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        val_acc = correct / total\n\n        epsilon = privacy_engine.get_epsilon(delta=1e-5)\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Val Acc: {val_acc:.4f} - ε: {epsilon:.2f}\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_state = model.state_dict()\n\n    # Load best model for test evaluation\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n        torch.save(model.state_dict(), \"best_dp_medical_model.pth\")\n        print(\"Final model saved as best_dp_medical_model.pth\")\n\n    # Test evaluation\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            preds = outputs.argmax(dim=1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n    test_acc = accuracy_score(y_true, y_pred)\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n\n    # Collect confidence scores for MIA\n    train_confidences = collect_confidences(model, train_loader)\n    test_confidences = collect_confidences(model, test_loader)\n\n    mia_X = np.concatenate([train_confidences, test_confidences])[:, None]\n    mia_y = np.concatenate([np.ones(len(train_confidences)), np.zeros(len(test_confidences))])\n    attack_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n    attack_model.fit(mia_X, mia_y)\n    mia_preds = attack_model.predict(mia_X)\n    mia_probs = attack_model.predict_proba(mia_X)[:, 1]\n    mia_acc = accuracy_score(mia_y, mia_preds)\n    mia_auc = roc_auc_score(mia_y, mia_probs)\n    print(f\"MIA Attack Accuracy: {mia_acc:.3f}, ROC AUC: {mia_auc:.3f}\")\n\n    # Clear memory to avoid crashes\n    del model, optimizer, privacy_engine, train_loader, val_loader, test_loader\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return best_val_acc, epsilon, mia_acc, mia_auc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:16:42.684718Z","iopub.execute_input":"2025-08-13T18:16:42.685415Z","iopub.status.idle":"2025-08-13T18:16:42.697800Z","shell.execute_reply.started":"2025-08-13T18:16:42.685391Z","shell.execute_reply":"2025-08-13T18:16:42.697040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train ONLY with the best settings!\nfinal_val_acc, final_epsilon, final_mia_acc, final_mia_auc = train_and_evaluate(\n    noise_multiplier=1.5,\n    max_grad_norm=1.0,\n    learning_rate=0.001,\n    epochs=15  # or 10/20 depending on what you want\n)\nprint(f\"\\nFinal Model - Val Acc: {final_val_acc:.4f}, Epsilon: {final_epsilon:.3f}, \"\n      f\"MIA Acc: {final_mia_acc:.3f}, MIA AUC: {final_mia_auc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:16:49.451274Z","iopub.execute_input":"2025-08-13T18:16:49.452015Z","iopub.status.idle":"2025-08-13T18:33:49.511436Z","shell.execute_reply.started":"2025-08-13T18:16:49.451989Z","shell.execute_reply":"2025-08-13T18:33:49.510666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below code is working on the imbalance in the dataset to get better results- That code is running a balanced Membership Inference Attack (MIA) test using the confidence scores your model outputs","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n\n# assumes you already have:\n# train_confidences, test_confidences = collect_confidences(model, loader) from your script\n\ndef evaluate_mia_balanced(train_confidences, test_confidences, seed=42):\n    rng = np.random.default_rng(seed)\n    n = min(len(train_confidences), len(test_confidences))\n\n    idx_tr = rng.choice(len(train_confidences), n, replace=False)\n    idx_te = rng.choice(len(test_confidences), n, replace=False)\n\n    X = np.concatenate([train_confidences[idx_tr], test_confidences[idx_te]])[:, None]\n    y = np.concatenate([np.ones(n), np.zeros(n)])\n\n    attack = LogisticRegression(solver='lbfgs', max_iter=1000)\n    attack.fit(X, y)\n    preds = attack.predict(X)\n    probs = attack.predict_proba(X)[:, 1]\n\n    acc = accuracy_score(y, preds)\n    auc = roc_auc_score(y, probs)\n    prec, rec, f1, _ = precision_recall_fscore_support(y, preds, average='binary')\n\n    print(f\"[Balanced MIA] Acc: {acc:.3f} | AUC: {auc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}\")\n\n# call it:\nevaluate_mia_balanced(train_confidences, test_confidences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:00:40.219235Z","iopub.execute_input":"2025-08-13T19:00:40.219865Z","iopub.status.idle":"2025-08-13T19:00:40.236752Z","shell.execute_reply.started":"2025-08-13T19:00:40.219838Z","shell.execute_reply":"2025-08-13T19:00:40.235633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\ndef mia_balanced_once(train_confidences, test_confidences, seed):\n    rng = np.random.default_rng(seed)\n    n = min(len(train_confidences), len(test_confidences))\n    X = np.concatenate([\n        rng.choice(train_confidences, n, replace=False),\n        rng.choice(test_confidences,  n, replace=False)\n    ])[:, None]\n    y = np.concatenate([np.ones(n), np.zeros(n)])\n    clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n    clf.fit(X, y)\n    probs = clf.predict_proba(X)[:,1]\n    preds = (probs >= 0.5).astype(int)\n    return accuracy_score(y, preds), roc_auc_score(y, probs)\n\nseeds = range(20)\naccs, aucs = zip(*(mia_balanced_once(train_confidences, test_confidences, s) for s in seeds))\nprint(f\"Balanced MIA (20 runs) — Acc: {np.mean(accs):.3f}±{np.std(accs):.3f}, AUC: {np.mean(aucs):.3f}±{np.std(aucs):.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:03:04.093781Z","iopub.execute_input":"2025-08-13T19:03:04.094317Z","iopub.status.idle":"2025-08-13T19:03:04.203510Z","shell.execute_reply.started":"2025-08-13T19:03:04.094297Z","shell.execute_reply":"2025-08-13T19:03:04.202942Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A balanced membership inference attack (MIA) was conducted over 20 independent runs to remove dataset size bias.\nThe attack achieved a mean accuracy of 0.540 ± 0.007 and a mean AUC of 0.517 ± 0.009.\nAs both values are close to the random-guessing baseline (0.5), this indicates that the model’s output confidences contain minimal distinguishable information about training set membership, suggesting low privacy leakage under this attack scenario.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# ================================\n# Shadow-Model MIA for Chest X-rays (ResNet18 + Opacus for target)\n# ================================\nimport os\nimport gc\nimport math\nimport numpy as np\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom torchvision import datasets, transforms, models\nfrom torchvision.models import ResNet18_Weights\n\nfrom opacus import PrivacyEngine\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n\n# -------------------------\n# Config\n# -------------------------\nBASE_DIR = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\nTRAIN_DIR = f\"{BASE_DIR}/train\"\nTEST_DIR  = f\"{BASE_DIR}/test\"\n\nBATCH_SIZE     = 32\nEPOCHS_TARGET  = 10\nEPOCHS_SHADOW  = 8\nLR             = 1e-3\nDP_NOISE       = 1.0      # DP noise (target only)\nMAX_GRAD_NORM  = 1.0\nVAL_RATIO_TGT  = 0.2      # % of target split for validation\nSEED           = 42\nDEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# -------------------------\n# Transforms\n# -------------------------\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),  # ResNet expects 3 channels\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# -------------------------\n# Data\n# -------------------------\nfull_train = datasets.ImageFolder(TRAIN_DIR, transform=transform)\ntest_ds    = datasets.ImageFolder(TEST_DIR,  transform=transform)\nclasses    = full_train.classes\nprint(f\"Classes: {classes}\")\n\n# Split full_train -> target_pool (60%), shadow_pool (40%)\ntgt_size   = int(0.6 * len(full_train))\nshd_size   = len(full_train) - tgt_size\ntarget_pool, shadow_pool = random_split(full_train, [tgt_size, shd_size], generator=torch.Generator().manual_seed(SEED))\n\n# Further split target_pool -> target_train (80%), target_val (20%)\ntgt_train_size = int((1.0 - VAL_RATIO_TGT) * len(target_pool))\ntgt_val_size   = len(target_pool) - tgt_train_size\ntarget_train, target_val = random_split(target_pool, [tgt_train_size, tgt_val_size], generator=torch.Generator().manual_seed(SEED))\n\nprint(f\"Target train: {len(target_train)} | Target val: {len(target_val)} | Shadow pool: {len(shadow_pool)} | Test: {len(test_ds)}\")\n\n# Shadow split -> shadow_train (80%), shadow_val (20%)\nshd_train_size = int(0.8 * len(shadow_pool))\nshd_val_size   = len(shadow_pool) - shd_train_size\nshadow_train, shadow_val = random_split(shadow_pool, [shd_train_size, shd_val_size], generator=torch.Generator().manual_seed(SEED))\nprint(f\"Shadow train: {len(shadow_train)} | Shadow val: {len(shadow_val)}\")\n\n# DataLoaders\ndef make_loader(ds, bs=BATCH_SIZE, shuffle=False):\n    return DataLoader(ds, batch_size=bs, shuffle=shuffle, num_workers=2, pin_memory=True)\n\ntgt_train_loader = make_loader(target_train, shuffle=True)\ntgt_val_loader   = make_loader(target_val)\ntest_loader      = make_loader(test_ds)\n\nshd_train_loader = make_loader(shadow_train, shuffle=True)\nshd_val_loader   = make_loader(shadow_val)\n\n# -------------------------\n# Models\n# -------------------------\ndef build_resnet18_head2():\n    m = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n    for p in m.parameters(): p.requires_grad = False\n    in_f = m.fc.in_features\n    m.fc = nn.Linear(in_f, 2)\n    for p in m.fc.parameters(): p.requires_grad = True\n    return m.to(DEVICE)\n\n# -------------------------\n# Train loops\n# -------------------------\ndef train_epoch(model, loader, optimizer, criterion):\n    model.train()\n    run_loss = 0.0\n    for x, y in loader:\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item()\n    return run_loss / max(1, len(loader))\n\n@torch.no_grad()\ndef eval_acc(model, loader):\n    model.eval()\n    correct = 0\n    total = 0\n    for x, y in loader:\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        preds = out.argmax(1)\n        correct += (preds == y).sum().item()\n        total += y.size(0)\n    return correct / max(1, total)\n\n# -------------------------\n# TARGET: DP training\n# -------------------------\ntarget_model = build_resnet18_head2()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(target_model.fc.parameters(), lr=LR)\n\nprivacy_engine = PrivacyEngine()\ntarget_model, optimizer, tgt_train_loader = privacy_engine.make_private(\n    module=target_model,\n    optimizer=optimizer,\n    data_loader=tgt_train_loader,\n    noise_multiplier=DP_NOISE,\n    max_grad_norm=MAX_GRAD_NORM,\n    epochs=EPOCHS_TARGET,   # for accounting\n)\n\nprint(\"\\n--- Training DP Target ---\")\nbest_val = 0.0\nbest_state = None\nfor ep in range(1, EPOCHS_TARGET+1):\n    loss = train_epoch(target_model, tgt_train_loader, optimizer, criterion)\n    val_acc = eval_acc(target_model, tgt_val_loader)\n    eps = privacy_engine.get_epsilon(delta=1e-5)\n    print(f\"Epoch {ep}/{EPOCHS_TARGET} - Loss: {loss:.4f} - Val Acc: {val_acc:.4f} - ε: {eps:.2f}\")\n    if val_acc > best_val:\n        best_val = val_acc\n        best_state = target_model.state_dict()\n\nif best_state:\n    target_model.load_state_dict(best_state)\ntorch.save(target_model.state_dict(), \"dp_target_model.pth\")\nprint(f\"Saved target model. Best Val Acc: {best_val:.4f}\")\n\n# Evaluate on test set (utility)\ntest_acc = eval_acc(target_model, test_loader)\nprint(f\"Target Test Accuracy: {test_acc:.4f}\")\n\n# -------------------------\n# SHADOW: Non-DP training (attacker’s proxy of your pipeline)\n# -------------------------\nshadow_model = build_resnet18_head2()\nshd_criterion = nn.CrossEntropyLoss()\nshd_optimizer = optim.Adam(shadow_model.fc.parameters(), lr=LR)\n\nprint(\"\\n--- Training Shadow (non-DP) ---\")\nbest_shd = 0.0\nbest_shd_state = None\nfor ep in range(1, EPOCHS_SHADOW+1):\n    loss = train_epoch(shadow_model, shd_train_loader, shd_optimizer, shd_criterion)\n    val_acc = eval_acc(shadow_model, shd_val_loader)\n    print(f\"[Shadow] Epoch {ep}/{EPOCHS_SHADOW} - Loss: {loss:.4f} - Val Acc: {val_acc:.4f}\")\n    if val_acc > best_shd:\n        best_shd = val_acc\n        best_shd_state = shadow_model.state_dict()\n\nif best_shd_state:\n    shadow_model.load_state_dict(best_shd_state)\ntorch.save(shadow_model.state_dict(), \"shadow_model.pth\")\nprint(f\"Saved shadow model. Best Shadow Val Acc: {best_shd:.4f}\")\n\n# -------------------------\n# Attack feature extraction\n# -------------------------\n@torch.no_grad()\ndef collect_features(model, loader):\n    \"\"\"\n    Returns:\n      feats: [N, 2 + 3] => [p0, p1, loss, entropy, max_prob]\n      labs : labels (not used by attacker except for ref)\n    \"\"\"\n    model.eval()\n    feats = []\n    labs  = []\n    ce = nn.CrossEntropyLoss(reduction=\"none\")\n    for x, y in loader:\n        x = x.to(DEVICE)\n        y = y.to(DEVICE)\n        logits = model(x)\n        probs  = torch.softmax(logits, dim=1)\n        # per-sample loss\n        losses = ce(logits, y)\n        # entropy\n        entropy = -(probs * torch.log(probs.clamp_min(1e-12))).sum(dim=1)\n        # max prob\n        maxprob, _ = probs.max(dim=1)\n        # features: full probs + loss + entropy + maxprob\n        feat_batch = torch.cat([probs, losses.unsqueeze(1), entropy.unsqueeze(1), maxprob.unsqueeze(1)], dim=1)\n        feats.append(feat_batch.cpu().numpy())\n        labs.extend(y.cpu().numpy())\n    feats = np.concatenate(feats, axis=0) if len(feats) else np.zeros((0, 5))\n    labs  = np.array(labs)\n    return feats, labs\n\n# Train attacker on shadow model signals:\n# member = shadow_train; non-member = shadow_val\nshadow_member_feats, _     = collect_features(shadow_model, shd_train_loader)\nshadow_nonmember_feats, _  = collect_features(shadow_model, shd_val_loader)\n\nX_attack_train = np.vstack([shadow_member_feats, shadow_nonmember_feats])\ny_attack_train = np.concatenate([np.ones(len(shadow_member_feats)), np.zeros(len(shadow_nonmember_feats))])\n\n# -------------------------\n# Fit attack model\n# -------------------------\nattack_clf = LogisticRegression(max_iter=2000, solver=\"lbfgs\")\nattack_clf.fit(X_attack_train, y_attack_train)\n\n# -------------------------\n# Evaluate attacker on TARGET model:\n# member = target_train; non-member = test\n# -------------------------\ntarget_member_feats, _    = collect_features(target_model, tgt_train_loader)\ntarget_nonmember_feats, _ = collect_features(target_model, test_loader)\n\nX_attack_test = np.vstack([target_member_feats, target_nonmember_feats])\ny_attack_test = np.concatenate([np.ones(len(target_member_feats)), np.zeros(len(target_nonmember_feats))])\n\nattack_preds = attack_clf.predict(X_attack_test)\nattack_probs = attack_clf.predict_proba(X_attack_test)[:, 1]\n\nmia_acc = accuracy_score(y_attack_test, attack_preds)\nmia_auc = roc_auc_score(y_attack_test, attack_probs)\n\nprint(\"\\n=== Shadow-Model MIA Results (on TARGET) ===\")\nprint(f\"Attack Accuracy: {mia_acc:.3f}\")\nprint(f\"Attack ROC AUC: {mia_auc:.3f}\")\n\n# Optional: quick report of target utility\n# (You can add a full classification_report using predictions if needed)\nprint(f\"\\nTarget test accuracy (utility): {test_acc:.3f} | Shadow val acc: {best_shd:.3f}\")\n\n# -------------------------\n# Clean up CUDA memory\n# -------------------------\ndel target_model, shadow_model, optimizer, shd_optimizer, privacy_engine\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:34:30.524327Z","iopub.execute_input":"2025-08-13T19:34:30.524866Z","iopub.status.idle":"2025-08-13T19:46:26.080734Z","shell.execute_reply.started":"2025-08-13T19:34:30.524838Z","shell.execute_reply":"2025-08-13T19:46:26.080145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, torch.nn as nn, torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nfrom opacus import PrivacyEngine\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nimport gc, math\n\n# ---------------------- Config ----------------------\nbase_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\ntrain_dir, test_dir = f\"{base_dir}/train\", f\"{base_dir}/test\"\nbatch_size = 32\nepochs = 12                 # will early-stop\nnoise_multiplier = 1.7      # ↑ for more privacy, try 2.0 as well\nmax_grad_norm = 0.5\nlr = 8e-4\nweight_decay = 5e-4\nlabel_smoothing = 0.1\nmixup_alpha = 0.4           # 0 disables mixup\npatience = 3                # early stop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ---------------------- Data -----------------------\ntfm = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\nfull_train = datasets.ImageFolder(train_dir, transform=tfm)\ntest_set   = datasets.ImageFolder(test_dir, transform=tfm)\ntrain_size = int(0.8 * len(full_train))\nval_size   = len(full_train) - train_size\ntrain_set, val_set = random_split(full_train, [train_size, val_size])\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\nclasses = full_train.classes\nprint(f\"Train: {len(train_set)} | Val: {len(val_set)} | Test: {len(test_set)} | Classes: {classes}\")\n\n# ---------------------- Model ----------------------\ndef build_model(dropout_p=0.5):\n    m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n    for p in m.parameters():\n        p.requires_grad = False\n    in_f = m.fc.in_features\n    m.fc = nn.Sequential(\n        nn.Dropout(dropout_p),\n        nn.Linear(in_f, 2),\n    )\n    return m.to(device)\n\n# ---------------------- MixUp ----------------------\ndef mixup_batch(x, y, alpha):\n    if alpha <= 0:\n        return x, y.float(), torch.ones(len(y), device=x.device)\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0), device=x.device)\n    x_mixed = lam * x + (1 - lam) * x[idx]\n    y_onehot = torch.nn.functional.one_hot(y, num_classes=2).float()\n    y_mixed = lam * y_onehot + (1 - lam) * y_onehot[idx]\n    return x_mixed, y_mixed, torch.full((len(y),), lam, device=x.device)\n\n# Soft CE to handle mixup + label smoothing in one go\nclass SoftTargetCrossEntropy(nn.Module):\n    def __init__(self, label_smoothing=0.0):\n        super().__init__()\n        self.eps = label_smoothing\n    def forward(self, logits, targets):  # targets: one-hot or soft labels (B, C)\n        # Smooth the targets\n        if targets.dim() == 1:\n            targets = torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float()\n        targets = (1 - self.eps) * targets + self.eps / logits.size(1)\n        log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n        return -(targets * log_probs).sum(dim=1).mean()\n\n# ---------------------- Train (DP) ----------------------\ndef train_dp_model():\n    model = build_model(dropout_p=0.5)\n    criterion = SoftTargetCrossEntropy(label_smoothing=label_smoothing)\n    optimizer = optim.Adam(model.fc.parameters(), lr=lr, weight_decay=weight_decay)\n\n    privacy_engine = PrivacyEngine()\n    model, optimizer, priv_train_loader = privacy_engine.make_private(\n        module=model,\n        optimizer=optimizer,\n        data_loader=train_loader,\n        noise_multiplier=noise_multiplier,\n        max_grad_norm=max_grad_norm,\n        epochs=epochs,\n    )\n\n    best_val = -1.0\n    best_state = None\n    patience_left = patience\n\n    for ep in range(1, epochs+1):\n        model.train()\n        run_loss, n = 0.0, 0\n        for x, y in priv_train_loader:\n            x, y = x.to(device), y.to(device)\n\n            # mixup\n            x_mix, y_soft, _ = mixup_batch(x, y, mixup_alpha)\n\n            optimizer.zero_grad()\n            logits = model(x_mix)\n            loss = criterion(logits, y_soft)\n            loss.backward()\n            optimizer.step()\n\n            b = x.size(0)\n            run_loss += loss.item() * b\n            n += b\n\n        # validation\n        model.eval()\n        y_true, y_pred, y_prob = [], [], []\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device), y.to(device)\n                logits = model(x)\n                probs = torch.softmax(logits, dim=1)\n                preds = probs.argmax(dim=1)\n                y_true.extend(y.cpu().numpy())\n                y_pred.extend(preds.cpu().numpy())\n                y_prob.extend(probs[:,1].cpu().numpy())\n        val_acc = accuracy_score(y_true, y_pred)\n        epsilon = privacy_engine.get_epsilon(delta=1e-5)\n        print(f\"Epoch {ep}/{epochs} - Loss: {run_loss/max(1,n):.4f} - Val Acc: {val_acc:.4f} - ε: {epsilon:.2f}\")\n\n        if val_acc > best_val + 1e-4:\n            best_val = val_acc\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n            patience_left = patience\n        else:\n            patience_left -= 1\n            if patience_left <= 0:\n                print(f\"Early stopping at epoch {ep}. Best Val Acc: {best_val:.4f}\")\n                break\n\n    # load best\n    if best_state is not None:\n        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n    torch.save(model.state_dict(), \"dp_resnet18_privacy_aware.pth\")\n    return model, epsilon\n\n# ---------------------- Evaluate ----------------------\ndef evaluate(model, loader, title=\"Eval\"):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            preds = logits.argmax(dim=1)\n            y_true.extend(y.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n    acc = accuracy_score(y_true, y_pred)\n    print(f\"{title} Accuracy: {acc:.4f}\")\n    print(classification_report(y_true, y_pred, target_names=classes, digits=3))\n    return acc\n\n# ---------------------- MIA: Balanced confidence attack ----------------------\ndef collect_confidences(model, loader):\n    model.eval()\n    conf = []\n    with torch.no_grad():\n        for x, _ in loader:\n            x = x.to(device)\n            probs = torch.softmax(model(x), dim=1)\n            maxp, _ = probs.max(dim=1)\n            conf.extend(maxp.cpu().numpy())\n    return np.array(conf)\n\ndef mia_balanced(train_conf, test_conf, runs=10, seed=42):\n    rng = np.random.default_rng(seed)\n    from sklearn.metrics import roc_auc_score\n    accs, aucs = [], []\n    for _ in range(runs):\n        n = min(len(train_conf), len(test_conf))\n        tr_idx = rng.choice(len(train_conf), n, replace=False)\n        te_idx = rng.choice(len(test_conf), n, replace=False)\n        X = np.concatenate([train_conf[tr_idx], test_conf[te_idx]])[:, None]\n        y = np.concatenate([np.ones(n), np.zeros(n)])\n        clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n        clf.fit(X, y)\n        preds = clf.predict(X)\n        probs = clf.predict_proba(X)[:,1]\n        accs.append(accuracy_score(y, preds))\n        aucs.append(roc_auc_score(y, probs))\n    return np.mean(accs), np.std(accs), np.mean(aucs), np.std(aucs)\n\n# ---------------------- Run ----------------------\nmodel, eps = train_dp_model()\nval_acc = evaluate(model, val_loader, title=\"Validation\")\ntest_acc = evaluate(model, test_loader, title=\"Test\")\n\ntr_conf = collect_confidences(model, train_loader)\nte_conf = collect_confidences(model, test_loader)\nacc_m, acc_sd, auc_m, auc_sd = mia_balanced(tr_conf, te_conf, runs=20)\nprint(f\"Balanced MIA (20 runs) — Acc: {acc_m:.3f}±{acc_sd:.3f}, AUC: {auc_m:.3f}±{auc_sd:.3f}\")\n\n# house-keeping\ngc.collect()\nif torch.cuda.is_available(): torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T20:01:20.904390Z","iopub.execute_input":"2025-08-13T20:01:20.905174Z","iopub.status.idle":"2025-08-13T20:15:00.401294Z","shell.execute_reply.started":"2025-08-13T20:01:20.905146Z","shell.execute_reply":"2025-08-13T20:15:00.400460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(model, loader, class_names):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            preds = torch.argmax(outputs, dim=1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    disp.plot(cmap=plt.cm.Blues, values_format='d')\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n# Example usage:\n# Define class names manually (order must match label encoding)\nclass_names = ['NORMAL', 'PNEUMONIA']\n\n# Validation confusion matrix\nplot_confusion_matrix(model, val_loader, class_names)\n\n# Test confusion matrix\nplot_confusion_matrix(model, test_loader, class_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T20:25:01.779472Z","iopub.execute_input":"2025-08-13T20:25:01.780017Z","iopub.status.idle":"2025-08-13T20:25:23.282321Z","shell.execute_reply.started":"2025-08-13T20:25:01.779992Z","shell.execute_reply":"2025-08-13T20:25:23.281487Z"}},"outputs":[],"execution_count":null}]}